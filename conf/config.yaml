# AE_LSTM project main configuration file

# model:
#   name: "lstm" # Default model to run. Can be overridden to 'mamba', 'transformer', etc.

# AE_LSTM project main configuration file


defaults:
  - _self_
  - data: default
  - model: default
  - train: default
  - paths: default 
  - visualization: default
  - evaluation: default
  - override hydra/sweeper: optuna
  # Changed line:
  - override model/model_configs@model.network_params: ${model.name}

model:
  name: "lstm" # Default model to run. Can be overridden to 'mamba', 'transformer', etc.

# Hydra configuration
hydra:
  run:
    # 修改：使用不依赖 ${paths.output_dir} 的相对路径或绝对路径
    dir: "./outputs/hydra_runs/${now:%Y-%m-%d}/${now:%H-%M-%S}_${hydra.job.name}"
  job:
    chdir: true # 通常建议为 true，这样脚本的相对路径基于原始工作目录
  sweep:
    # 修改：使用不依赖 ${paths.output_dir} 的相对路径或绝对路径
    dir: "./outputs/hydra_multirun/${now:%Y-%m-%d}/${now:%H-%M-%S}"
    subdir: ${hydra.job.num} # ${hydra.job.num} 是 Hydra 提供的，通常没问题

  sweeper:
    study_name: "ae_lstm_manual_sweep"      # 修改：硬编码
    direction: minimize
    n_trials: 2                             # 为了测试，保持较小值
    # storage: "sqlite:////tmp/my_optuna_study.db" # 尝试 /tmp 目录 # 修改：硬编码，确保 ./outputs 目录存在
    storage: null
    sampler:
      _target_: optuna.samplers.TPESampler
      seed: 123                             # 修改：硬编码
    params:
      # Autoencoder Model Params - 简化搜索
      model.autoencoder.latent_dim: choice(32, 64)  # 尝试两个常见的潜在维度
      model.autoencoder.hidden_layers: choice("[256, 128]", "[128, 64]") # 确保能被正确解析
      model.autoencoder.dropout_rate: float(0.1, 0.3) # 较小的 dropout 范围

      # LSTM Model Params - 简化搜索
      model.lstm.hidden_size: choice(64, 128)      # 尝试两个常见的 LSTM 隐藏层大小
      model.lstm.num_layers: choice(1, 2)           # 1层或2层 LSTM
      model.lstm.dropout: float(0.1, 0.3)          # 较小的 LSTM dropout 范围
      # Training Params - 简化学习率和 batch_size 搜索
      # Autoencoder Training
      train.autoencoder.learning_rate: float(1e-4, 1e-3) # 较窄的学习率范围
      # train.autoencoder.optimizer.weight_decay: float(1e-5, 1e-4, log=True) # 暂时固定或移除
      train.autoencoder.batch_size: choice(32, 64)

      # LSTM Training
      train.lstm.learning_rate: float(1e-4, 1e-3) # 较窄的学习率范围
      # train.lstm.optimizer.weight_decay: float(1e-5, 1e-4, log=True) # 暂时固定或移除
      train.lstm.batch_size: choice(32, 64)

    # Data Params - 固定序列长度或选择较少选项
      data.sequence.sequence_length: choice(7, 10) # 尝试两个序列长度 

# Experiment settings (这里的插值通常在应用代码中解析，问题不大)
experiment:
  name: "ae_lstm_salinity_prediction"
  seed: 42
  device: "cuda"

# Logging configuration (这里的插值也主要在应用代码中解析)
logging:
  level: "INFO"
  save_dir: ${paths.output_dir}/logs # 如果这里也导致问题，可以考虑也硬编码为 "./outputs/logs"
  use_tensorboard: true
  use_wandb: false
  wandb_project: "ae_lstm_salinity"
  wandb_entity: null
  log_batch_metrics: true