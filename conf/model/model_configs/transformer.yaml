# @package _group_
transformer:
  # name: "transformer"
  input_size: 32    # Set dynamically
  output_size: 32   # Set dynamically
  d_model: 64       # Internal dimension of the transformer
  nhead: 4          # Number of attention heads
  num_encoder_layers: 3
  dim_feedforward: 128
  dropout: 0.1
  sequence_length: 10 # Max sequence length for positional encoding
