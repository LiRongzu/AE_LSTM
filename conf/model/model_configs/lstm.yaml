# @package _group_
lstm:
  # name: "lstm"
  # LSTM specific parameters
  type: "standard" # standard, bidirectional, attention
  input_size: 32   # Should match autoencoder latent_dim (set dynamically in pipeline)
  hidden_size: 32
  num_layers: 1
  output_size: 32  # Should match autoencoder latent_dim (set dynamically in pipeline)
  dropout: 0.1
  sequence_length: 10
# batch_size, learning_rate, weight_decay are in train config (e.g., conf/train/lstm.yaml)
